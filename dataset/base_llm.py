from datasets import Dataset, DatasetDict
from typing import Optional, Sequence, List, Dict
import os
from datasets import load_dataset, concatenate_datasets
import json, random
import transformers
from pathlib import Path
import omegaconf
from tqdm import tqdm
# srun -p mllm-align --quotatype=reserved --gres=gpu:1 --cpus-per-task=16 --time=3000 python dataset/base.py
rejection_templates = [
    "Sorry, I cannot fulfill that request.",
    "I am not able to provide an answer to this.",
    "Unfortunately, I cannot comply with that.",
    "Iâ€™m afraid I cannot answer that question.",
    "I cannot reveal that information.",
    "I'm not able to disclose those details.",
    "I cannot share that with you.",
    "It is not safe to provide that information.",
    "I'm afraid I don't have access to that data.",
    "This question is not appropriate for me to answer."
]


system_prompt: str = "You are a helpful and harmless assistant."
user_cot_prompt_template: str = (
    "You are given a question, You should think step-by-step and answer the question.\n"
    "Question: {A}\n\n"
)
user_prompt_template: str = (
    "You are given a question, please answer the question.\n"
    "Question: {A}\n\n"
)

generate_cot_prompt_template: str = (
    "You are given a question. Please provide a very brief explanation why it may potentially cause {A}, considering the user intent.\n"
    "Question: {B}\n\n"
)
     

    
class AdvBenchDataset(Dataset):
    think_model: bool = True
    def __init__(self,task_configs,num_samples,think_mode) -> None:
        self.task_configs= task_configs
        self.num_samples = num_samples
        self.dataset = load_dataset("walledai/AdvBench")
        self.think_mode = think_mode
        breakpoint()
    def get_reject_dataset(self):
        def convert_to_llama(row):
            for dict in row["conversations"]:
                if dict["from"] == "human":
                    questions = user_cot_prompt_template.format(A=dict["value"]) if self.think_mode else user_prompt_template.format(A=dict["value"])
                elif dict["from"] == "gpt":
                    answers = dict["value"]
            return {
                "question": questions,
                "label": answers
            }
        self.dataset = self.dataset.map(convert_to_llama).remove_columns(["id","conversations"])
        self.dataset = self.dataset.train_test_split(test_size=0.1)
  
          
class GeneralDataset(Dataset):
    think_mode: bool  = True
    def __init__(self,num_samples,think_mode) -> None:
        self.num_samples = num_samples
        self.think_mode = think_mode
        self.dataset = load_dataset("walledai/AdvBench")
        breakpoint()
    def get_reject_dataset(self):
        def convert_to_llama(row):
            for dict in row["conversations"]:
                if dict["from"] == "human":
                    questions = user_cot_prompt_template.format(A=dict["value"]) if self.think_mode else user_prompt_template.format(A=dict["value"])
                elif dict["from"] == "gpt":
                    answers = dict["value"]
            return {
                "question": questions,
                "label": answers
            }
        self.dataset = self.dataset.map(convert_to_llama).remove_columns(["id","conversations"])
        self.dataset = self.dataset.train_test_split(test_size=0.1)
        return self.dataset
    
   
class XSTest(Dataset):
    '''
    XSTest comprises:
        -250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with.
        -200 unsafe prompts as contrasts that, for most LLM applications, should be refused.
    
    The test suite highlights systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.
    '''
    think_model: bool = True
    def __init__(self,num_samples,think_mode) -> None:
        self.dataset = load_dataset("walledai/XSTest")
        self.num_samples = num_samples
        self.think_mode = think_mode
        # self.dataset = self.dataset.shuffle(seed=42).select(range(self.num_samples))
        
    def get_dataset(self):
        self.dataset = self.dataset.map(
            lambda row: {
                "question": user_cot_prompt_template.format(A=row["prompt"]),
                "label": row["label"]
            })
        # ).remove_columns(["id","image","conversations"])
        
        return self.dataset
            

class StrongREJECT(Dataset):
    '''
    A novel benchmark of 313 malicious prompts for use in evaluating jailbreaking attacks against LLMs, aimed to expose whether a jailbreak attack actually enables malicious actors to utilize LLMs for harmful tasks.
    '''
    think_model: bool = True
    def __init__(self,num_samples,think_mode) -> None:
        self.dataset = load_dataset("walledai/StrongREJECT")
        self.num_samples = num_samples
        self.think_mode = think_mode
        # self.dataset = self.dataset.shuffle(seed=42).select(range(self.num_samples))
        
    def get_dataset(self):
        self.dataset = self.dataset.map(
            lambda row: {
                "question": user_cot_prompt_template.format(A=row["prompt"]),
                "label": "unsafe"
            })
        
        return self.dataset
   


if __name__ == "__main__":
    prompt_config_path = 'config/cot_prompt_llm.yaml'
    task_configs = omegaconf.OmegaConf.load(prompt_config_path)
    # xstest = load_dataset("walledai/XSTest")
    breakpoint()
    advbench = AdvBenchDataset(task_configs=task_configs,num_samples=2000,think_mode=True)
    ds_adv = advbench.get_reject_dataset()
    breakpoint()
    
    general_ds = GeneralDataset(num_samples=5000,think_mode=True)
    gen_ds = general_ds.get_reject_dataset()
    # xstest = XSTest(task_configs=task_configs.mm_safetybench)
    breakpoint()
    # reject_dataset = xstest.get_dataset()
    # breakpoint()
    # reject_cot_dataset = xstest.get_cot_reject_dataset()
