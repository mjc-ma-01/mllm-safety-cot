  0%|          | 0/492 [00:00<?, ?it/s]It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
 41%|████      | 200/492 [01:51<02:35,  1.88it/s]/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
{'loss': 2.1193, 'grad_norm': 43.23298263549805, 'learning_rate': 1.9634146341463414e-05, 'mean_token_accuracy': 0.5732665419578552, 'epoch': 0.04}
{'loss': 1.581, 'grad_norm': 4.195060729980469, 'learning_rate': 1.9227642276422765e-05, 'mean_token_accuracy': 0.6197485208511353, 'epoch': 0.08}
{'loss': 1.4261, 'grad_norm': 3.230600357055664, 'learning_rate': 1.8821138211382116e-05, 'mean_token_accuracy': 0.6434583544731141, 'epoch': 0.12}
{'loss': 1.2777, 'grad_norm': 5.5339579582214355, 'learning_rate': 1.8414634146341467e-05, 'mean_token_accuracy': 0.6617545247077942, 'epoch': 0.16}
{'loss': 1.1645, 'grad_norm': 4.85753870010376, 'learning_rate': 1.8008130081300814e-05, 'mean_token_accuracy': 0.6902453064918518, 'epoch': 0.2}
{'loss': 1.0423, 'grad_norm': 2.3450241088867188, 'learning_rate': 1.7601626016260165e-05, 'mean_token_accuracy': 0.7041570127010346, 'epoch': 0.24}
{'loss': 1.0596, 'grad_norm': 4.326612949371338, 'learning_rate': 1.7195121951219512e-05, 'mean_token_accuracy': 0.707185971736908, 'epoch': 0.28}
{'loss': 1.0049, 'grad_norm': 3.5370492935180664, 'learning_rate': 1.6788617886178863e-05, 'mean_token_accuracy': 0.7135824024677276, 'epoch': 0.33}
{'loss': 0.9878, 'grad_norm': 7.224311828613281, 'learning_rate': 1.6382113821138214e-05, 'mean_token_accuracy': 0.7114540934562683, 'epoch': 0.37}
{'loss': 0.9472, 'grad_norm': 5.256119251251221, 'learning_rate': 1.597560975609756e-05, 'mean_token_accuracy': 0.7177540361881256, 'epoch': 0.41}
{'loss': 0.9123, 'grad_norm': 7.970110893249512, 'learning_rate': 1.5569105691056912e-05, 'mean_token_accuracy': 0.7348813772201538, 'epoch': 0.45}
{'loss': 0.8872, 'grad_norm': 2.9602549076080322, 'learning_rate': 1.5162601626016263e-05, 'mean_token_accuracy': 0.7357330858707428, 'epoch': 0.49}
{'loss': 0.8841, 'grad_norm': 2.3500664234161377, 'learning_rate': 1.475609756097561e-05, 'mean_token_accuracy': 0.7387418568134307, 'epoch': 0.53}
{'loss': 0.8673, 'grad_norm': 2.431406259536743, 'learning_rate': 1.4349593495934961e-05, 'mean_token_accuracy': 0.7418351650238038, 'epoch': 0.57}
{'loss': 0.8691, 'grad_norm': 2.9915435314178467, 'learning_rate': 1.394308943089431e-05, 'mean_token_accuracy': 0.7368437707424164, 'epoch': 0.61}
{'loss': 0.8456, 'grad_norm': 3.9977970123291016, 'learning_rate': 1.3536585365853661e-05, 'mean_token_accuracy': 0.7364360213279724, 'epoch': 0.65}
{'loss': 0.8157, 'grad_norm': 6.865747451782227, 'learning_rate': 1.3130081300813009e-05, 'mean_token_accuracy': 0.7581331729888916, 'epoch': 0.69}
{'loss': 0.8303, 'grad_norm': 5.63936185836792, 'learning_rate': 1.272357723577236e-05, 'mean_token_accuracy': 0.7397324919700623, 'epoch': 0.73}
{'loss': 0.8371, 'grad_norm': 2.894268751144409, 'learning_rate': 1.2317073170731709e-05, 'mean_token_accuracy': 0.7410412967205048, 'epoch': 0.77}
{'loss': 0.7821, 'grad_norm': 2.663039207458496, 'learning_rate': 1.191056910569106e-05, 'mean_token_accuracy': 0.7526241183280945, 'epoch': 0.81}
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
 81%|████████▏ | 400/492 [03:49<00:48,  1.89it/s]/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
{'loss': 0.8326, 'grad_norm': 4.122715950012207, 'learning_rate': 1.1504065040650407e-05, 'mean_token_accuracy': 0.7505525290966034, 'epoch': 0.85}
{'loss': 0.7832, 'grad_norm': 5.088540077209473, 'learning_rate': 1.1097560975609758e-05, 'mean_token_accuracy': 0.7525225043296814, 'epoch': 0.89}
{'loss': 0.8319, 'grad_norm': 4.641040802001953, 'learning_rate': 1.0691056910569107e-05, 'mean_token_accuracy': 0.7466511368751526, 'epoch': 0.93}
{'loss': 0.7767, 'grad_norm': 5.327013969421387, 'learning_rate': 1.0284552845528458e-05, 'mean_token_accuracy': 0.7569973230361938, 'epoch': 0.98}
{'loss': 0.7794, 'grad_norm': 3.6093995571136475, 'learning_rate': 9.878048780487805e-06, 'mean_token_accuracy': 0.7605680406093598, 'epoch': 1.02}
{'loss': 0.7605, 'grad_norm': 3.636385440826416, 'learning_rate': 9.471544715447156e-06, 'mean_token_accuracy': 0.7599013805389404, 'epoch': 1.06}
{'loss': 0.7665, 'grad_norm': 3.265223264694214, 'learning_rate': 9.065040650406505e-06, 'mean_token_accuracy': 0.7563164710998536, 'epoch': 1.1}
{'loss': 0.7677, 'grad_norm': 3.9680845737457275, 'learning_rate': 8.658536585365854e-06, 'mean_token_accuracy': 0.7601942002773285, 'epoch': 1.14}
{'loss': 0.794, 'grad_norm': 2.4984679222106934, 'learning_rate': 8.252032520325203e-06, 'mean_token_accuracy': 0.7553771018981934, 'epoch': 1.18}
{'loss': 0.7319, 'grad_norm': 4.686022758483887, 'learning_rate': 7.845528455284554e-06, 'mean_token_accuracy': 0.7630964398384095, 'epoch': 1.22}
{'loss': 0.7965, 'grad_norm': 3.8721346855163574, 'learning_rate': 7.439024390243903e-06, 'mean_token_accuracy': 0.7469676434993744, 'epoch': 1.26}
{'loss': 0.789, 'grad_norm': 3.245802640914917, 'learning_rate': 7.032520325203252e-06, 'mean_token_accuracy': 0.758893620967865, 'epoch': 1.3}
{'loss': 0.767, 'grad_norm': 6.103259563446045, 'learning_rate': 6.626016260162602e-06, 'mean_token_accuracy': 0.7635700762271881, 'epoch': 1.34}
{'loss': 0.7634, 'grad_norm': 12.444689750671387, 'learning_rate': 6.219512195121951e-06, 'mean_token_accuracy': 0.7533291935920715, 'epoch': 1.38}
{'loss': 0.733, 'grad_norm': 3.5830399990081787, 'learning_rate': 5.813008130081301e-06, 'mean_token_accuracy': 0.7654642581939697, 'epoch': 1.42}
{'loss': 0.794, 'grad_norm': 9.434972763061523, 'learning_rate': 5.4065040650406504e-06, 'mean_token_accuracy': 0.7512273669242859, 'epoch': 1.46}
{'loss': 0.7521, 'grad_norm': 2.3164258003234863, 'learning_rate': 5e-06, 'mean_token_accuracy': 0.7544487237930297, 'epoch': 1.5}
{'loss': 0.7362, 'grad_norm': 3.2959630489349365, 'learning_rate': 4.59349593495935e-06, 'mean_token_accuracy': 0.7632542550563812, 'epoch': 1.54}
{'loss': 0.7669, 'grad_norm': 19.189483642578125, 'learning_rate': 4.1869918699186995e-06, 'mean_token_accuracy': 0.7576481461524963, 'epoch': 1.59}
{'loss': 0.7328, 'grad_norm': 5.381805896759033, 'learning_rate': 3.780487804878049e-06, 'mean_token_accuracy': 0.7625948905944824, 'epoch': 1.63}
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
100%|██████████| 492/492 [04:49<00:00,  1.93it/s]/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
{'loss': 0.7051, 'grad_norm': 3.186516761779785, 'learning_rate': 3.3739837398373986e-06, 'mean_token_accuracy': 0.7697326719760895, 'epoch': 1.67}
{'loss': 0.7337, 'grad_norm': 3.7799556255340576, 'learning_rate': 2.967479674796748e-06, 'mean_token_accuracy': 0.7645014345645904, 'epoch': 1.71}
{'loss': 0.7093, 'grad_norm': 3.0032217502593994, 'learning_rate': 2.5609756097560977e-06, 'mean_token_accuracy': 0.7734313368797302, 'epoch': 1.75}
{'loss': 0.7838, 'grad_norm': 2.880929946899414, 'learning_rate': 2.154471544715447e-06, 'mean_token_accuracy': 0.752614551782608, 'epoch': 1.79}
{'loss': 0.7378, 'grad_norm': 3.0385656356811523, 'learning_rate': 1.747967479674797e-06, 'mean_token_accuracy': 0.7612740755081177, 'epoch': 1.83}
{'loss': 0.711, 'grad_norm': 3.0298445224761963, 'learning_rate': 1.3414634146341465e-06, 'mean_token_accuracy': 0.7717828929424286, 'epoch': 1.87}
{'loss': 0.7125, 'grad_norm': 2.1601974964141846, 'learning_rate': 9.349593495934959e-07, 'mean_token_accuracy': 0.7649648606777191, 'epoch': 1.91}
{'loss': 0.7454, 'grad_norm': 4.491121292114258, 'learning_rate': 5.284552845528456e-07, 'mean_token_accuracy': 0.7608400344848633, 'epoch': 1.95}
{'loss': 0.7466, 'grad_norm': 3.2659926414489746, 'learning_rate': 1.2195121951219514e-07, 'mean_token_accuracy': 0.7606817007064819, 'epoch': 1.99}
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
100%|██████████| 492/492 [04:59<00:00,  1.64it/s]
{'train_runtime': 301.5011, 'train_samples_per_second': 13.015, 'train_steps_per_second': 1.632, 'train_loss': 0.8807540928445211, 'mean_token_accuracy': 0.7699437439441681, 'epoch': 2.0}
/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
