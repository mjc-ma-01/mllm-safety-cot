  0%|          | 0/1260 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
                                                    
{'loss': 1.4699, 'grad_norm': nan, 'learning_rate': 9.92857142857143e-05, 'mean_token_accuracy': 0.11838625073432922, 'epoch': 0.02}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.84920634920635e-05, 'mean_token_accuracy': 0.00013440860202535988, 'epoch': 0.05}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.76984126984127e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.07}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.69047619047619e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.1}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.611111111111112e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.12}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.531746031746033e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.14}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.452380952380952e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.17}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.373015873015874e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.19}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.293650793650794e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.21}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.214285714285714e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.24}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.134920634920635e-05, 'mean_token_accuracy': 0.0001168224262073636, 'epoch': 0.26}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.055555555555556e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.29}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.976190476190477e-05, 'mean_token_accuracy': 0.00010010009864345193, 'epoch': 0.31}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.896825396825397e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.33}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.817460317460317e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.36}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.738095238095239e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.38}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.658730158730159e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.4}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.579365079365079e-05, 'mean_token_accuracy': 0.0005042710923589766, 'epoch': 0.43}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.5e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.45}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.420634920634921e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.48}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.341269841269842e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.5}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.261904761904762e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.52}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.182539682539683e-05, 'mean_token_accuracy': 8.169934735633433e-05, 'epoch': 0.55}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.103174603174604e-05, 'mean_token_accuracy': 6.882312591187656e-05, 'epoch': 0.57}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.023809523809524e-05, 'mean_token_accuracy': 0.00010000000474974513, 'epoch': 0.6}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.944444444444444e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.62}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.865079365079366e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.64}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.785714285714286e-05, 'mean_token_accuracy': 0.00010834236163645983, 'epoch': 0.67}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.706349206349208e-05, 'mean_token_accuracy': 8.389261784031987e-05, 'epoch': 0.69}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.626984126984126e-05, 'mean_token_accuracy': 0.00010351967066526412, 'epoch': 0.71}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.547619047619048e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.74}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.468253968253969e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.76}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.38888888888889e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.79}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.30952380952381e-05, 'mean_token_accuracy': 0.0, 'epoch': 0.81}
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/random.py", line 195, in fork_rng
    yield
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1519, in recompute_fn
    fn(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 339, in forward
    hidden_states = hidden_states + self.attn(
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 302, in forward
    attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/petrelfs/majiachen/project/mllm-safety-cot/src/sft_vlm_cot_.py", line 306, in <module>
    trainer.train()
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 261, in backward
    self.engine.backward(loss, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2053, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2062, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1125, in unpack_hook
    frame.recompute_fn(*args)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1507, in recompute_fn
    with torch.random.fork_rng(
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/random.py", line 199, in fork_rng
    device_mod.set_rng_state(device_rng_state, device)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/random.py", line 77, in set_rng_state
    _lazy_call(cb)
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/__init__.py", line 249, in _lazy_call
    callable()
  File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/random.py", line 75, in cb
    default_generator.set_state(new_state_copy)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/random.py", line 195, in fork_rng
[rank0]:     yield
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1519, in recompute_fn
[rank0]:     fn(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 339, in forward
[rank0]:     hidden_states = hidden_states + self.attn(
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 302, in forward
[rank0]:     attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True
[rank0]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/petrelfs/majiachen/project/mllm-safety-cot/src/sft_vlm_cot_.py", line 306, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 261, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2053, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2062, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1125, in unpack_hook
[rank0]:     frame.recompute_fn(*args)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1507, in recompute_fn
[rank0]:     with torch.random.fork_rng(
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/contextlib.py", line 153, in __exit__
[rank0]:     self.gen.throw(typ, value, traceback)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/random.py", line 199, in fork_rng
[rank0]:     device_mod.set_rng_state(device_rng_state, device)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/random.py", line 77, in set_rng_state
[rank0]:     _lazy_call(cb)
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/__init__.py", line 249, in _lazy_call
[rank0]:     callable()
[rank0]:   File "/mnt/petrelfs/majiachen/miniconda3/envs/v-oocr/lib/python3.10/site-packages/torch/cuda/random.py", line 75, in cb
[rank0]:     default_generator.set_state(new_state_copy)
[rank0]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
